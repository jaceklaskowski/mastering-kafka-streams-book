== [[StreamThread]] StreamThread -- Stream Processor Thread

`StreamThread` is a *stream processor thread* (a Java https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Thread.html[Thread]) that <<runLoop, runs the main record processing loop>> when <<run, started>>.

.StreamThread and Stream Processing
image::images/kafka-streams-StreamThread.png[align="center"]

`StreamThread` is <<create, created>> exclusively alongside <<kafka-streams-KafkaStreams.adoc#creating-instance, KafkaStreams>> (which is one of the main entities that a Kafka Streams developer uses in a Kafka Streams application).

NOTE: `KafkaStreams` uses <<kafka-streams-KafkaStreams.adoc#threads, num.stream.threads>> configuration property for the number of `StreamThreads` to create (default: `1`).

`StreamThread` uses a <<consumer, Kafka Consumer>> and a custom <<rebalanceListener, ConsumerRebalanceListener>> (with the <<taskManager, TaskManager>>) when subscribing to source topics (when requested to <<runLoop, run the main record processing loop>> and <<enforceRebalance, enforce a rebalance>>). `StreamThread` uses an <<builder, InternalTopologyBuilder>> for the <<kafka-streams-internals-InternalTopologyBuilder.adoc#sourceTopicPattern, source topics>> to subscribe to.

NOTE: When <<kafka-streams-StreamThread-RebalanceListener.adoc#onPartitionsAssigned, partitions get assigned>>, the <<rebalanceListener, custom ConsumerRebalanceListener>> requests the `TaskManager` to <<kafka-streams-internals-TaskManager.adoc#createTasks, create tasks for the assigned partitions>>.

`StreamThread` uses a <<consumer, Kafka Consumer>> and a <<taskManager, TaskManager>> that are both created when `StreamThread` object is requested to <<create, create an instance of itself>>. That is when the `StreamThread` sets the <<kafka-streams-StreamsConfig.adoc#TASK_MANAGER_FOR_PARTITION_ASSIGNOR, TASK_MANAGER_FOR_PARTITION_ASSIGNOR>> internal property and indirectly associates the <<taskManager, TaskManager>> with <<kafka-streams-internals-StreamsPartitionAssignor.adoc#, StreamsPartitionAssignor>>. `StreamThread` also requests the given <<kafka-streams-KafkaClientSupplier.adoc#, KafkaClientSupplier>> to <<getConsumer, create a KafkaConsumer>> (with the <<taskManager, TaskManager>>) and so when the <<kafka-streams-internals-StreamsPartitionAssignor.adoc#PARTITION_ASSIGNMENT_STRATEGY_CONFIG, partition.assignment.strategy>> configuration property is picked up, `StreamsPartitionAssignor` is <<kafka-streams-internals-StreamsPartitionAssignor.adoc#, created>> and eventually <<kafka-streams-internals-StreamsPartitionAssignor.adoc#configure, configured>> (that will use the <<kafka-streams-StreamsConfig.adoc#TASK_MANAGER_FOR_PARTITION_ASSIGNOR, TASK_MANAGER_FOR_PARTITION_ASSIGNOR>> internal property).

.StreamThread and Registering TaskManager under TASK_MANAGER_FOR_PARTITION_ASSIGNOR
image::images/kafka-streams-StreamThread-TASK_MANAGER_FOR_PARTITION_ASSIGNOR.png[align="center"]

[[pollTime]]
`StreamThread` uses the <<kafka-streams-properties.adoc#poll.ms, poll.ms>> configuration property (default: `100` ms) as the *polling interval* when requested to <<runOnce, poll records once and process them using active stream tasks>>.

[[commitTimeMs]]
`StreamThread` uses the <<kafka-streams-properties.adoc#commit.interval.ms, commit.interval.ms>> configuration property as the *flush interval* for persisting the position of a processor (when <<runOnce, polling records once and processing them using active stream tasks>> and <<maybeCommit, maybeCommit>>).

`StreamThread` uses the <<consumer, Kafka Consumer>> to:

* Subscribe to topics (with <<rebalanceListener, RebalanceListener>>) right after `StreamThread` has been requested to <<runLoop, run the main record processing loop>>

* Poll the topics subscribed (and fetch records if available) right after `StreamThread` has been requested to <<pollRequests, get the next batch of records by polling>>

* <<resetInvalidOffsets, resetInvalidOffsets>> (when an `InvalidOffsetException` is reported while <<pollRequests, polling the topics for records>>)

`StreamThread` uses *stream-thread [[clientId]-StreamThread-[STREAM_THREAD_ID]]* for the logging prefix.

`StreamThread` requires an <<builder, InternalTopologyBuilder>> to be <<create, created>> and uses it for the following:

* Creating a <<kafka-streams-internals-TaskCreator.adoc#, TaskCreator>> and a <<kafka-streams-internals-StandbyTaskCreator.adoc#, StandbyTaskCreator>>

* <<runLoop, Running the main record processing loop>> (and subscribing to the <<kafka-streams-internals-InternalTopologyBuilder.adoc#sourceTopicPattern, source topics>>)

* <<resetInvalidOffsets, resetInvalidOffsets>>

`StreamThread` uses the <<taskManager, TaskManager>> for the following:

* FIXME

[[logging]]
[TIP]
====
Enable any of `ALL` logging levels for `org.apache.kafka.streams.processor.internals.StreamThread` logger to see what happens inside.

Add the following line to `log4j.properties`:

```
log4j.logger.org.apache.kafka.streams.processor.internals.StreamThread=ALL
```

Refer to <<kafka-logging.adoc#log4j.properties, Application Logging Using log4j>>.
====

=== [[state]][[State]] Life Cycle of StreamThread -- StreamThread's States

`StreamThread` can be in exactly one of the following *states* at any given point in time:

[start=0]
1. [[CREATED]] `CREATED` - The initial state of `StreamThread` right after it was <<creating-instance, created>>
1. [[RUNNING]] `RUNNING` - `StreamThread` was requested for the following:
* <<run, run>>
* <<runOnce, Polling records once and processing them using active stream tasks>> when `StreamThread` is in <<PARTITIONS_ASSIGNED, PARTITIONS_ASSIGNED>> state and <<taskManager, TaskManager>> was positive after link:kafka-streams-internals-TaskManager.adoc#updateNewAndRestoringTasks[updateNewAndRestoringTasks]
* <<runOnce, Polling records once and processing them using active stream tasks>> when `StreamThread` polled for records and happened to transition to <<PARTITIONS_ASSIGNED, PARTITIONS_ASSIGNED>> state, but (again) only when <<taskManager, TaskManager>> was positive after link:kafka-streams-internals-TaskManager.adoc#updateNewAndRestoringTasks[updateNewAndRestoringTasks]
1. [[STARTING]] `STARTING`
1. [[PARTITIONS_REVOKED]] `PARTITIONS_REVOKED` - `RebalanceListener` was requested to link:kafka-streams-StreamThread-RebalanceListener.adoc#onPartitionsRevoked[handle partition revocation]
1. [[PARTITIONS_ASSIGNED]] `PARTITIONS_ASSIGNED` - `RebalanceListener` was requested to link:kafka-streams-StreamThread-RebalanceListener.adoc#onPartitionsAssigned[handle partition assignment]
1. [[PENDING_SHUTDOWN]] `PENDING_SHUTDOWN` - `StreamThread` was requested to <<shutdown, shutdown>> or <<completeShutdown, completeShutdown>>
1. [[DEAD]] `DEAD` - `StreamThread` is requested to <<completeShutdown, completeShutdown>>

.StreamThread's Life Cycle
image::images/kafka-streams-StreamThread-lifecycle.png[align="center"]

`StreamThread` can be transitioned to another state by executing <<setState, setState>>.

NOTE: `StreamThread` defines a Java enumeration `State` with the states above ordered by ordinal. When created, they are assigned the state ordinals that could transition to. You can check whether a transition is valid or not using `State.isValidTransition`.

[source, scala]
----
import org.apache.kafka.streams.processor.internals.StreamThread.State._

// CREATED is the 0th state
assert(CREATED.ordinal == 0)

// RUNNING is the next possible state after CREATED
assert(CREATED.isValidTransition(RUNNING))

// DEAD cannot the next possible state after CREATED
assert(CREATED.isValidTransition(DEAD) == false)
----

=== [[pollRequests]] Polling Records -- `pollRequests` Internal Method

[source, java]
----
ConsumerRecords<byte[], byte[]> pollRequests(Duration pollTime)
----

`pollRequests` simply requests the <<consumer, Kafka Consumer>> to poll record with the given `pollTime`.

In case of an `InvalidOffsetException`, `pollRequests` <<resetInvalidOffsets, resetInvalidOffsets>>.

In case of a <<rebalanceException, rebalanceException>>, `pollRequests` re-throws it as a `TaskMigratedException` or a `StreamsException`.

NOTE: `pollRequests` is used exclusively when `StreamThread` is requested to <<runOnce, poll records once and process them using active stream tasks>>.

=== [[resetInvalidOffsets]] `resetInvalidOffsets` Internal Method

[source, java]
----
void resetInvalidOffsets(final InvalidOffsetException e)
----

`resetInvalidOffsets`...FIXME

NOTE: `resetInvalidOffsets` is used exclusively when `StreamThread` is requested to <<pollRequests, pollRequests>> (and an `InvalidOffsetException` is reported).

=== [[completeShutdown]] `completeShutdown` Internal Method

[source, java]
----
void completeShutdown(final boolean cleanRun)
----

`completeShutdown`...FIXME

NOTE: `completeShutdown` is used when `StreamThread` is requested to <<run, run>> and <<shutdown, shutdown>>.

=== [[shutdown]] `shutdown` Method

[source, java]
----
void shutdown()
----

`shutdown`...FIXME

NOTE: `shutdown` is used exclusively when `KafkaStreams` is requested to link:kafka-streams-KafkaStreams.adoc#close[close].

=== [[runOnce]] Polling Records Once And Processing Them Using Active Stream Tasks -- `runOnce` Method

[source, java]
----
void runOnce()
----

In essence, `runOnce` requests the <<consumer, Consumer>> to poll records, <<addRecordsToTasks, adds the records to active stream tasks>> and requests the <<taskManager, TaskManager>> to <<kafka-streams-internals-TaskManager.adoc#process, process the records by running stream tasks>>.

.StreamThread and Polling Records Once And Processing Them Using Active Stream Tasks
image::images/kafka-streams-StreamThread-runOnce.png[align="center"]

NOTE: `runOnce` uses the <<streamsMetrics, StreamsMetricsThreadImpl>> to access <<kafka-streams-StreamsMetricsThreadImpl.adoc#sensors, sensors>> and record metrics.

Internally, `runOnce` <<pollRequests, pollRequests>> with different poll times as follows:

* `0L` when in <<PARTITIONS_ASSIGNED, PARTITIONS_ASSIGNED>> state

* <<pollTime, pollTime>> when in <<PARTITIONS_REVOKED, PARTITIONS_REVOKED>>, <<STARTING, STARTING>> or <<RUNNING, RUNNING>> state

[NOTE]
====
When in the other <<state, states>> (when <<pollRequests, pollRequests>> above), `runOnce` prints out the following ERROR message to the logs followed by throwing a `StreamsException`:

```
Unexpected state [state] during normal iteration
```
====

`runOnce` <<advanceNowAndComputeLatency, advanceNowAndComputeLatency>>.

With records polled, `runOnce` requests the <<streamsMetrics, StreamsMetricsThreadImpl>> for the <<kafka-streams-StreamsMetricsThreadImpl.adoc#pollTimeSensor, pollTimeSensor>> and requests it to record the above `pollLatency` followed by <<addRecordsToTasks, adding the records polled to active stream tasks>>.

If in <<PARTITIONS_ASSIGNED, PARTITIONS_ASSIGNED>> state, `runOnce` requests the <<taskManager, TaskManager>> to <<kafka-streams-internals-TaskManager.adoc#updateNewAndRestoringTasks, updateNewAndRestoringTasks>> and (when all stream tasks are running) changes to the <<RUNNING, RUNNING>> state.

`runOnce` <<advanceNowAndComputeLatency, advanceNowAndComputeLatency>>.

`runOnce` requests the <<taskManager, TaskManager>> to <<kafka-streams-internals-TaskManager.adoc#hasActiveRunningTasks, check out if hasActiveRunningTasks>> and if so...FIXME

In the end, `runOnce` <<maybeUpdateStandbyTasks, maybeUpdateStandbyTasks>> followed by <<maybeCommit, maybeCommit>>.

NOTE: `runOnce` is used exclusively when `StreamThread` is requested to <<runLoop, run the main record processing loop>>.

=== [[maybeUpdateStandbyTasks]] Attempting to Update Running StandbyTasks -- `maybeUpdateStandbyTasks` Internal Method

[source, java]
----
void maybeUpdateStandbyTasks()
----

`maybeUpdateStandbyTasks`...FIXME

`maybeUpdateStandbyTasks` does nothing and simply returns when `StreamThread` is not in <<RUNNING, RUNNING>> state or the <<taskManager, TaskManager>> has no <<kafka-streams-internals-TaskManager.adoc#hasStandbyRunningTasks, hasStandbyRunningTasks>>.

NOTE: `maybeUpdateStandbyTasks` is used exclusively when `StreamThread` is requested to <<runOnce, poll records once and process them using active stream tasks>>.

=== [[creating-instance]] Creating StreamThread Instance

`StreamThread` takes the following to be created:

* [[time]] `Time`
* [[config]] <<kafka-streams-StreamsConfig.adoc#, StreamsConfig>>
* [[producer]] Kafka http://kafka.apache.org/22/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html[Producer] (`Producer<byte[], byte[]>`)
* [[restoreConsumer]] Kafka "restore" https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[Consumer] (`Consumer<byte[], byte[]>`)
* [[consumer]] Kafka https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[Consumer] (`Consumer<byte[], byte[]>`)
* [[originalReset]] `originalReset`
* [[taskManager]] <<kafka-streams-internals-TaskManager.adoc#, TaskManager>>
* [[streamsMetrics]] <<kafka-streams-StreamsMetricsThreadImpl.adoc#, StreamsMetricsThreadImpl>>
* [[builder]] <<kafka-streams-internals-InternalTopologyBuilder.adoc#, InternalTopologyBuilder>>
* [[threadClientId]] `threadClientId`
* [[logContext]] `LogContext`
* [[assignmentErrorCode]] `assignmentErrorCode`

`StreamThread` initializes the <<internal-properties, internal properties>>.

=== [[runLoop]] Running Main Record Processing Loop -- `runLoop` Internal Method

[source, java]
----
void runLoop()
----

`runLoop` simply requests the <<consumer, Consumer>> to subscribe to the <<kafka-streams-internals-InternalTopologyBuilder.adoc#sourceTopicPattern, source topics>> (with the custom <<rebalanceListener, ConsumerRebalanceListener>>) and keeps <<runOnce, polling records and processing them using active stream tasks>> until the <<isRunning, isRunning>> flag is off.

.StreamThread and Running Main Record Processing Loop
image::images/kafka-streams-StreamThread-runLoop.png[align="center"]

`runLoop` requests the <<consumer, Consumer>> to subscribe to the <<kafka-streams-internals-InternalTopologyBuilder.adoc#sourceTopicPattern, source topics>> (from the <<builder, InternalTopologyBuilder>>) with the custom <<rebalanceListener, ConsumerRebalanceListener>>.

`runLoop` then keeps <<runOnce, polling records and processing them using active stream tasks>> until the <<isRunning, isRunning>> flag is off.

In case of the <<assignmentErrorCode, assignmentErrorCode>> set to `VERSION_PROBING`, `runLoop` prints out the following INFO message to the logs followed by <<enforceRebalance, enforcing a rebalance>>.

```
Version probing detected. Triggering new rebalance.
```

In case of `TaskMigratedException`, `runLoop` prints out the following WARN message to the logs followed by <<enforceRebalance, enforcing a rebalance>>.

```
Detected task [taskId] that got migrated to another thread. This implies that this thread missed a rebalance and dropped out of the consumer group. Will try to rejoin the consumer group. Below is the detailed description of the task:
[migratedTask]
```

NOTE: `runLoop` is used exclusively when `StreamThread` is requested to <<run, start>>.

=== [[run]] Starting Stream Processor Thread -- `run` Method

[source, java]
----
void run()
----

NOTE: `run` is part of Java's https://docs.oracle.com/javase/10/docs/api/java/lang/Thread.html#run--[Thread Contract] to be executed by a JVM thread.

`run` prints out the following INFO message to the logs.

```
Starting
```

`run` <<setState, sets the state>> to <<RUNNING, RUNNING>> and <<runLoop, runs the main record processing loop>>.

At the end, `run` <<completeShutdown, shuts down>> (per `cleanRun` flag that says whether <<runLoop, running the main loop>> stopped cleanly or not).

`run` re-throws any `KafkaException`.

`run` prints out the following ERROR message to the logs for any other `Exception`:

```
Encountered the following error during processing: [exception]
```

NOTE: `run` is used exclusively when `KafkaStreams` is requested to <<kafka-streams-KafkaStreams.adoc#start, start>>.

=== [[setState]] Setting New State -- `setState` Method

[source, java]
----
boolean setState(final State newState)
----

`setState`...FIXME

NOTE: `setState` is used when...FIXME

=== [[setRebalanceException]] `setRebalanceException` Method

[source, java]
----
void setRebalanceException(final Throwable rebalanceException)
----

`setRebalanceException`...FIXME

NOTE: `setRebalanceException` is used when...FIXME

=== [[adjustRecordsProcessedBeforeCommit]] Adjusting Number Of Records Processed Before Commit (Per Current Processing and Commit Times) -- `adjustRecordsProcessedBeforeCommit` Internal Method

[source, java]
----
long adjustRecordsProcessedBeforeCommit(
  final long prevRecordsProcessedBeforeCommit,
  final long totalProcessed,
  final long processLatency,
  final long commitTime)
----

`adjustRecordsProcessedBeforeCommit` adjusts (scales up or down) the <<recordsProcessedBeforeCommit, number of records process before commit>> per current `processLatency` and `commitTime` times.

If `processLatency` is greater than `0` and `commitTime`, `adjustRecordsProcessedBeforeCommit` scales `recordsProcessedBeforeCommit` down and prints out the following DEBUG message to the logs:

```
processing latency [processLatency] > commit time [commitTime] for [totalProcessed] records. Adjusting down recordsProcessedBeforeCommit=[recordsProcessedBeforeCommit]
```

If however `prevRecordsProcessedBeforeCommit` is available (measured) and `processLatency` is greater than `0` (but not `commitTime`), `adjustRecordsProcessedBeforeCommit` scales `recordsProcessedBeforeCommit` up and prints out the following DEBUG message to the logs:

```
processing latency [processLatency] < commit time [commitTime] for [totalProcessed] records. Adjusting up recordsProcessedBeforeCommit=[recordsProcessedBeforeCommit]
```

NOTE: `adjustRecordsProcessedBeforeCommit` is used exclusively when `StreamThread` is requested to <<runOnce, poll records once and process them using active stream tasks>> (and there have been records available).

=== [[toString]] Describing Itself (Textual Representation) -- `toString` Method

[source, java]
----
String toString() // <1>
String toString(final String indent)
----
<1> Calls `toString(final String indent)` with an empty indent, i.e. `""`

`toString` gives a text representation with "StreamsThread threadId:" and the thread name followed by the link:kafka-streams-internals-TaskManager.adoc#toString[text representation] of the <<taskManager, TaskManager>>.

[source, scala]
----
FIXME toString in action
----

=== [[processAndMaybeCommit]] Processing Records (with Optional Commit) -- `processAndMaybeCommit` Internal Method

[source, java]
----
long processAndMaybeCommit(final long recordsProcessedBeforeCommit)
----

`processAndMaybeCommit` repeats the following processing loop until the number of active stream tasks that processed records successfully is 0 (i.e. no record has been processed).

.StreamThread.processAndMaybeCommit, TaskManager and Sensors
image::images/kafka-streams-StreamThread-processAndMaybeCommit.png[align="center"]

NOTE: `processAndMaybeCommit` is used exclusively when `StreamThread` is requested to <<runOnce, poll records once and process them using active stream tasks>> (and there have been records to be processed).

==== [[processAndMaybeCommit-processing-loop]] Processing Loop of `processAndMaybeCommit`

`processAndMaybeCommit` requests <<taskManager, TaskManager>> to link:kafka-streams-internals-TaskManager.adoc#process[process records] (by all active stream tasks).

If the number of active stream tasks that processed records successfully is greater than 0, `processAndMaybeCommit` requests process time sensor to record processing time (as <<computeLatency, computeLatency>> by the number of stream tasks that processed records).

NOTE: `processAndMaybeCommit` uses <<streamsMetrics, StreamsMetricsThreadImpl>> to access the link:kafka-streams-StreamsMetricsThreadImpl.adoc#processTimeSensor[process time sensor].

`processAndMaybeCommit` increments the total number of processed records by the number of active stream tasks processed records.

`processAndMaybeCommit` <<punctuate, punctuate>>.

`processAndMaybeCommit` resets the total number of processed records followed by <<maybeCommit, maybeCommit>> when all the following conditions are met:

1. The input `recordsProcessedBeforeCommit` is not `UNLIMITED_RECORDS`
1. The total number of processed records is greater than the input `recordsProcessedBeforeCommit`

`processAndMaybeCommit` requests <<taskManager, TaskManager>> to link:kafka-streams-internals-TaskManager.adoc#maybeCommitActiveTasks[maybeCommitActiveTasks].

If the number of active stream tasks that committed records is greater than 0, `processAndMaybeCommit` requests commit time sensor to record commit time (as <<computeLatency, computeLatency>> by the number of stream tasks that committed records).

NOTE: `processAndMaybeCommit` uses <<streamsMetrics, StreamsMetricsThreadImpl>> to access the link:kafka-streams-StreamsMetricsThreadImpl.adoc#commitTimeSensor[commit time sensor].

=== [[UNLIMITED_RECORDS]][[recordsProcessedBeforeCommit]] Tracking Number Of Records Processed Before Commit and `UNLIMITED_RECORDS` Marker

`StreamThread` tracks the *number of records processed before commit* (aka `recordsProcessedBeforeCommit`).

`StreamThread` uses `UNLIMITED_RECORDS` marker (i.e. `-1`) as the "zero" to say that no record has been processed yet.

[source, java]
----
private final static int UNLIMITED_RECORDS = -1;
----

When `StreamThread` is <<run, started>> (and <<runLoop, runs the main record processing loop>>), the number of records processed before commit is `UNLIMITED_RECORDS` (which makes it clear that the `StreamThread` has just been started and there has been no way that any record could have been processed).

The main event loop <<runOnce, polls records once and processes them using active stream tasks>> and gives the number of records processed in this one-off poll that becomes the number of records processed before commit from the moment in time on.

The number of records processed before commit may have changed but could still be `UNLIMITED_RECORDS` for the following:

1. There were no records to be polled
1. FIXME Anything else (that could have happened in <<runOnce, runOnce>>)?

CAUTION: FIXME Review <<runOnce, runOnce>> yet again to focus on `UNLIMITED_RECORDS`.

At some point `StreamThread` <<processAndMaybeCommit, may want to commit>> the records that have been processed so far that happens when the number of records processed before commit is no longer `UNLIMITED_RECORDS` and...FIXME

When requested for <<adjustRecordsProcessedBeforeCommit, adjusting the number of records processed before commit>>, `StreamThread` uses `UNLIMITED_RECORDS` for a local `recordsProcessedBeforeCommit` that is then scaled up or down per the current processing and commit times.

NOTE: <<adjustRecordsProcessedBeforeCommit, Adjusting the number of records processed before commit>> can only scale up when the current number of records processed before commit (while <<runOnce, polling records once and processing them using active stream tasks>>) is not `UNLIMITED_RECORDS` (and the processing time is not greater than commit time).

=== [[isRunning]] Checking If StreamThread Is Running -- `isRunning` Method

[source, java]
----
boolean isRunning()
----

`isRunning` is `true` when `StreamThread` is in one of the following <<state, states>>:

* <<RUNNING, RUNNING>>
* <<STARTING, STARTING>>
* <<PARTITIONS_REVOKED, PARTITIONS_REVOKED>>
* <<PARTITIONS_ASSIGNED, PARTITIONS_ASSIGNED>>

Otherwise, `isRunning` is `false`.

NOTE: `isRunning` is simply a pass-through variant of <<State, State.isRunning>>.

[NOTE]
====
`isRunning` is used when:

* `StreamThread` is requested to <<runLoop, run the main record processing loop>>

* `KafkaStreams` is requested to link:kafka-streams-KafkaStreams.adoc#close[close].
====

=== [[addRecordsToTasks]] Adding Records to Active Stream Tasks -- `addRecordsToTasks` Internal Method

[source, java]
----
void addRecordsToTasks(ConsumerRecords<byte[], byte[]> records)
----

For every https://kafka.apache.org/22/javadoc/org/apache/kafka/common/TopicPartition.html[partition] of the input link:++https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/consumer/ConsumerRecords.html#partitions--++[records] `addRecordsToTasks` requests the <<taskManager, TaskManager>> for the <<kafka-streams-internals-TaskManager.adoc#activeTask, active stream processor task>> responsible for the partition.

NOTE: The input records may (and often will) be from different partitions or even topics. Unless you use as many `StreamThread` instances as there are partitions (among the source topics), `addRecordsToTasks` will be given records from many partitions.

With the <<kafka-streams-internals-StreamTask.adoc#, StreamTask>>, `addRecordsToTasks` requests the input mixed-partition `ConsumerRecords` for the link:++https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/consumer/ConsumerRecords.html#records-org.apache.kafka.common.TopicPartition-++[records for the given partition only] and then requests the `StreamTask` to <<kafka-streams-internals-StreamTask.adoc#addRecords, buffer the new records>> (for the partition).

.StreamThread and Adding Records to Active Stream Tasks
image::images/kafka-streams-StreamThread-addRecordsToTasks.png[align="center"]

NOTE: https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/consumer/ConsumerRecords.html[ConsumerRecords] is a container that holds the list of `ConsumerRecord` per partition for a particular topic. There is one `ConsumerRecord` list for every topic partition returned by a `Consumer.poll(long)` operation.

NOTE: `addRecordsToTasks` is used exclusively when `StreamThread` is requested to <<runOnce, poll records once and process them using active stream tasks>>.

=== [[create]] Creating StreamThread Instance -- `create` Factory Method

[source, java]
----
StreamThread create(
  final InternalTopologyBuilder builder,
  final StreamsConfig config,
  final KafkaClientSupplier clientSupplier,
  final AdminClient adminClient,
  final UUID processId,
  final String clientId,
  final Metrics metrics,
  final Time time,
  final StreamsMetadataState streamsMetadataState,
  final long cacheSizeBytes,
  final StateDirectory stateDirectory,
  final StateRestoreListener userStateRestoreListener)
----

.Creating StreamThread
image::images/kafka-streams-StreamThread-create.png[align="center"]

`create` prints out the following INFO message to the logs:

```
Creating restore consumer client
```

`create` requests the input `StreamsConfig` for link:kafka-streams-StreamsConfig.adoc#getRestoreConsumerConfigs[getRestoreConsumerConfigs] for a new `threadClientId` (of the format `[clientId]-StreamThread-[STREAM_THREAD_ID]`).

`create` requests the given `KafkaClientSupplier` for link:kafka-streams-KafkaClientSupplier.adoc#getRestoreConsumer[getRestoreConsumer] for the `restoreConsumerConfigs`.

[[create-StoreChangelogReader]]
`create` creates a <<kafka-streams-StoreChangelogReader.adoc#, StoreChangelogReader>> (with the `restoreConsumer`, the given <<kafka-streams-StateRestoreListener.adoc#, StateRestoreListener>> and the configured <<kafka-streams-properties.adoc#poll.ms, poll.ms>>).

NOTE: The input <<kafka-streams-StateRestoreListener.adoc#, StateRestoreListener>> is a <<kafka-streams-DelegatingStateRestoreListener.adoc#, DelegatingStateRestoreListener>> actually.

(Only with eos enabled) `create`...FIXME

`create` creates a link:kafka-streams-StreamsMetricsThreadImpl.adoc#creating-instance[StreamsMetricsThreadImpl] with the following:

* the input `Metrics`
* *stream-metrics* link:kafka-streams-StreamsMetricsThreadImpl.adoc#groupName[group name]
* `thread.[clientId]-StreamThread-[STREAM_THREAD_ID]` link:kafka-streams-StreamsMetricsThreadImpl.adoc#prefix[prefix]
* link:kafka-streams-StreamsMetricsThreadImpl.adoc#tags[Tags] with one entry with `client-id` and the `[clientId]-StreamThread-[STREAM_THREAD_ID]` value.

`create` creates a <<kafka-streams-internals-ThreadCache.adoc#, ThreadCache>> (with `cacheSizeBytes` for the `maxCacheSizeBytes` and the `StreamsMetricsThreadImpl`).

`create` creates a link:kafka-streams-internals-TaskCreator.adoc#creating-instance[TaskCreator] and a link:kafka-streams-internals-StandbyTaskCreator.adoc#creating-instance[StandbyTaskCreator] that are used exclusively to create a <<kafka-streams-internals-TaskManager.adoc#, TaskManager>> (with a new link:kafka-streams-internals-AssignedStreamsTasks.adoc#creating-instance[AssignedStreamsTasks] and link:kafka-streams-AssignedStandbyTasks.adoc#creating-instance[AssignedStandbyTasks] as well as the given <<kafka-streams-StreamsMetadataState.adoc#, StreamsMetadataState>>).

`create` prints out the following INFO message to the logs:

```
Creating consumer client
```

`create` requests the input `StreamsConfig` for link:kafka-streams-properties.adoc#application.id[application.id] configuration property.

`create` requests the input `StreamsConfig` for the link:kafka-streams-StreamsConfig.adoc#getMainConsumerConfigs[configuration of a Kafka Consumer] for the application ID and the `threadClientId` (of the format `[clientId]-StreamThread-[STREAM_THREAD_ID]`) and adds the following internal properties:

* <<kafka-streams-StreamsConfig.adoc#TASK_MANAGER_FOR_PARTITION_ASSIGNOR, TASK_MANAGER_FOR_PARTITION_ASSIGNOR>> to be the `TaskManager` just created

* <<kafka-streams-StreamsConfig.adoc#ASSIGNMENT_ERROR_CODE, ASSIGNMENT_ERROR_CODE>> to be a new `AtomicInteger`

(Only with non-empty `latestResetTopicsPattern` and `earliestResetTopicsPattern` patterns) `create`...FIXME

`create` requests the given `KafkaClientSupplier` for a link:kafka-streams-KafkaClientSupplier.adoc#getConsumer[Kafka Consumer] (with the `consumerConfigs`) and link:kafka-streams-internals-TaskManager.adoc#setConsumer[associates] it with the `TaskManager`.

In the end, `create` creates a <<creating-instance, StreamThread>>.

NOTE: `create` is used exclusively when `KafkaStreams` is <<kafka-streams-KafkaStreams.adoc#, created>>.

=== [[enforceRebalance]] Enforcing Rebalance -- `enforceRebalance` Internal Method

[source, java]
----
void enforceRebalance()
----

`enforceRebalance`...FIXME

NOTE: `enforceRebalance` is used when...FIXME

=== [[maybeCommit]] Committing All Tasks (When Commit Interval Elapsed) -- `maybeCommit` Method

[source, java]
----
void maybeCommit(final long now)
----

`maybeCommit` commits all tasks (owned by this <<taskManager, TaskManager>>) if the <<commitTimeMs, commit interval>> has elapsed (i.e. the commit interval is non-negative and the time since the <<lastCommitMs, last commit>> is long gone).

`maybeCommit` prints out the following TRACE message to the logs:

```
Committing all active tasks [activeTaskIds] and standby tasks [standbyTaskIds] since [time]ms has elapsed (commit interval is [commitTimeMs]ms)
```

`maybeCommit` requests the <<taskManager, TaskManager>> to <<kafka-streams-internals-TaskManager.adoc#commitAll, commitAll>>.

Only if there are still running active and standby tasks, `maybeCommit` does the following:

. Requests the <<streamsMetrics, StreamsMetricsThreadImpl>> for the <<kafka-streams-StreamsMetricsThreadImpl.adoc#commitTimeSensor, commitTimeSensor>> and records the commit time (as the latency of committing all the tasks by their number)

. Requests the <<taskManager, TaskManager>> to <<kafka-streams-internals-TaskManager.adoc#maybePurgeCommitedRecords, maybePurgeCommitedRecords>>

`maybeCommit` prints out the following DEBUG message to the logs:

```
Committed all active tasks [activeTaskIds] and standby tasks [standbyTaskIds] in [duration]ms
```

`maybeCommit` updates the <<lastCommitMs, lastCommitMs>> internal counter with the input `now` time.

`maybeCommit` turns the <<processStandbyRecords, processStandbyRecords>> flag on.

NOTE: `maybeCommit` is used when `StreamThread` is requested to <<runOnce, poll records once and process them using active stream tasks>> (directly and indirectly in <<processAndMaybeCommit, processAndMaybeCommit>>).

=== [[maybePunctuate]] Attempting to Punctuateâ€‰(Running Stream Tasks) -- `maybePunctuate` Internal Method

[source, java]
----
boolean maybePunctuate()
----

`maybePunctuate` requests the <<taskManager, TaskManager>> to <<kafka-streams-internals-TaskManager.adoc#punctuate, punctuate stream tasks>>.

If the punctuate returned a positive number (greater than `0`), `maybePunctuate` <<advanceNowAndComputeLatency, advanceNowAndComputeLatency>> and requests the <<streamsMetrics, StreamsMetricsThreadImpl>> for the <<kafka-streams-StreamsMetricsThreadImpl.adoc#punctuateTimeSensor, punctuateTimeSensor>> to record the punctuate time.

In the end, `maybePunctuate` returns whether the punctuate returned a positive number (`true`) or not (`false`).

NOTE: `maybePunctuate` is used exclusively when `StreamThread` is requested to <<runOnce, poll records once and process them using active stream tasks>>.

== [[advanceNowAndComputeLatency]] Computing Latency -- `advanceNowAndComputeLatency` Internal Method

[source, java]
----
long advanceNowAndComputeLatency()
----

`advanceNowAndComputeLatency` updates (_advances_) the <<now, "now" timestamp>> to be the current timestamp and returns the timestamp difference (_latency_).

NOTE: `advanceNowAndComputeLatency` is used when `StreamThread` is requested to <<runOnce, poll records once and process them using active stream tasks>>, <<maybePunctuate, maybePunctuate>>, <<maybeCommit, maybeCommit>>, and <<maybeUpdateStandbyTasks, maybeUpdateStandbyTasks>>.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| builder
a| [[builder]] <<kafka-streams-internals-InternalTopologyBuilder.adoc#, InternalTopologyBuilder>>

| lastCommitMs
a| [[lastCommitMs]] Time of the last <<maybeCommit, commit>>

| numIterations
a| [[numIterations]] Number of iterations when the <<taskManager, TaskManager>> is requested to <<kafka-streams-internals-TaskManager.adoc#process, process records by running stream tasks (one record per task)>> (while `StreamThread` is <<runOnce, polling records once and processing them using active stream tasks>>)

Default: `1`

Incremented while <<runOnce, polling records once and processing them using active stream tasks>>

Decremented by half while <<runOnce, polling records once and processing them using active stream tasks>>

| processStandbyRecords
a| [[processStandbyRecords]] Flag to control whether to <<maybeUpdateStandbyTasks, maybeUpdateStandbyTasks>> after <<maybeCommit, maybeCommit>>

Default: `false`

Turned off (`false`) in <<maybeUpdateStandbyTasks, maybeUpdateStandbyTasks>> (after requesting the `StandbyTasks` to <<kafka-streams-internals-StandbyTask.adoc#update, update>>)

Turned on (`true`) when <<maybeCommit, attempting to commit>> (and the time to commit has come per <<commitTimeMs, commit.interval.ms>> configuration property)

| now
a| [[now]] *"now" timestamp*

| rebalanceListener
a| [[rebalanceListener]] <<kafka-streams-StreamThread-RebalanceListener.adoc#, RebalanceListener>>

* Used exclusively when `StreamThread` is requested to <<runLoop, run the main record processing loop>> (and requests the Kafka <<consumer, Consumer>> to subscribe to get dynamically assigned partitions of topics matching specified pattern)

NOTE: `StreamThread` requests <<builder, InternalTopologyBuilder>> for the link:kafka-streams-internals-InternalTopologyBuilder.adoc#sourceTopicPattern[source topic pattern] to subscribe to.

| stateListener
a| [[stateListener]][[setStateListener]] <<kafka-streams-StreamThread-StateListener.adoc#, StateListener>>

Used when `StreamThread` is requested to <<setState, change a state>>

Set when `KafkaStreams` is <<kafka-streams-KafkaStreams.adoc#creating-instance, created>>

Reset (`null`) when:

* `KafkaStreams` is requested to <<kafka-streams-KafkaStreams.adoc#close, close>>

* `RebalanceListener` is requested to <<kafka-streams-StreamThread-RebalanceListener.adoc#onPartitionsAssigned, handle a partition assignment>> (and there was `INCOMPLETE_SOURCE_TOPIC_METADATA` error)

| timerStartedMs
a| [[timerStartedMs]] The timestamp when the timer has started
|===
